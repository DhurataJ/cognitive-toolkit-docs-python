### YamlMime:PythonModule
uid: cntk.contrib.deeprl.agent.shared.customized_models
name: customized_models
fullName: cntk.contrib.deeprl.agent.shared.customized_models
summary: 'Customized Q function or (unnormalized) log of policy function.



  If models from cntk.contrib.deeprl.agent.shared.models are not adequate, write

  your own model as a function, which takes two required arguments

  ''shape_of_inputs'', ''number_of_outputs'', and two optional arguments

  ''loss_function'', ''use_placeholder_for_input'', and outputs a dictionary

  containing ''inputs'', ''outputs'', ''f'' and ''loss''. In the config file, set

  QRepresentation or PolicyRepresentation to path (module_name.function_name) of

  the function. QLearning/PolicyGradient will then automatically search for it.'
functions:
- uid: cntk.contrib.deeprl.agent.shared.customized_models.conv_dqn
  name: conv_dqn
  summary: "Example convolutional neural network for approximating the Q value function.\n\
    \nThis is the model used in the original DQN paper\n[https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf](https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf).\n\
    \nReturns: a Python dictionary with string-valued keys including\n   'inputs',\
    \ 'outputs', 'loss' and 'f'."
  signature: conv_dqn(shape_of_inputs, number_of_outputs, loss_function=None, use_placeholder_for_input=False)
  parameters:
  - name: shape_of_inputs
    description: tuple of array (input) dimensions.
  - name: number_of_outputs
    description: 'dimension of output, equals the number of

      possible actions.'
  - name: loss_function
    description: if not specified, use squared loss by default.
    defaultValue: None
  - name: use_placeholder_for_input
    description: 'if true, inputs have to be replaced

      later with actual input_variable.'
    defaultValue: 'False'

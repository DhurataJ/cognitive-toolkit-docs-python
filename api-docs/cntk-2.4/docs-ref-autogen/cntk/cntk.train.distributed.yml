### YamlMime:PythonModule
uid: cntk.train.distributed
name: distributed
fullName: cntk.train.distributed
summary: Distributed learners manage learners in distributed environment.
functions:
- uid: cntk.train.distributed.block_momentum_distributed_learner
  name: block_momentum_distributed_learner
  summary: 'Creates a block momentum distributed learner. See [1] for more

    information.

    Block Momentum divides the full dataset into M non-overlapping blocks,

    and each block is partitioned into N non-overlapping splits.

    During training, a random, unprocessed block is randomly taken by the trainer

    and the N partitions of this block are dispatched on the workers.'
  signature: block_momentum_distributed_learner(learner, block_size, block_momentum_as_time_constant=None,
    use_nestrov_momentum=True, reset_sgd_momentum_after_aggregation=True, block_learning_rate=1.0,
    distributed_after=0)
  parameters:
  - name: learner
    description: a local learner (i.e. sgd)
    isRequired: true
  - name: block_size
    description: size of the partition in samples
    isRequired: true
    types:
    - <xref:int>
  - name: block_momentum_as_time_constant
    description: block momentum as time constant
    isRequired: true
    types:
    - <xref:float>
  - name: use_nestrov_momentum
    description: use nestrov momentum
    isRequired: true
    types:
    - <xref:bool>
  - name: reset_sgd_momentum_after_aggregation
    description: reset SGD momentum after aggregation
    isRequired: true
    types:
    - <xref:bool>
  - name: block_learning_rate
    description: block learning rate
    isRequired: true
    types:
    - <xref:float>
  - name: distributed_after
    description: number of samples after which distributed training starts
    isRequired: true
    types:
    - <xref:int>
  return:
    description: a distributed learner instance
  seeAlsoContent: "  [1] K. Chen and Q. Huo. [Scalable training of deep learning machines\n\
    \  by incremental block training with intra-block parallel optimization\n  and\
    \ blockwise model-update filtering](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/08/0005880.pdf).\n\
    \  Proceedings of ICASSP, 2016.\n"
- uid: cntk.train.distributed.data_parallel_distributed_learner
  name: data_parallel_distributed_learner
  summary: Creates a data parallel distributed learner
  signature: data_parallel_distributed_learner(learner, distributed_after=0, num_quantization_bits=32,
    use_async_buffered_parameter_update=False)
  parameters:
  - name: learner
    description: a local learner (i.e. sgd)
    isRequired: true
  - name: distributed_after
    description: number of samples after which distributed training starts
    isRequired: true
    types:
    - <xref:int>
  - name: num_quantization_bits
    description: number of bits for quantization (1 to 32)
    isRequired: true
    types:
    - <xref:int>
  - name: use_async_buffered_parameter_update
    description: use async buffered parameter update, currently must be False
    isRequired: true
    types:
    - <xref:bool>
  return:
    description: a distributed learner instance
- uid: cntk.train.distributed.mpi_communicator
  name: mpi_communicator
  summary: Creates a non quantized MPI communicator.
  signature: mpi_communicator()
classes:
- cntk.train.distributed.Communicator
- cntk.train.distributed.DistributedLearner
- cntk.train.distributed.WorkerDescriptor
